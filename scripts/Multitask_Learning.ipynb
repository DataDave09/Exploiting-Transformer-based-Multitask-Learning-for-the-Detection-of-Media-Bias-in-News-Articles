{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Multitask_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "135ae31a37ff48e0aba2c851916292f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fba165ba3a1a4a119622b2ed30959a30",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_915f955db1264c47bc3c0b36f4ffb72f",
              "IPY_MODEL_392003aab3b2423cbc3fbdbe493e0aff"
            ]
          }
        },
        "fba165ba3a1a4a119622b2ed30959a30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "915f955db1264c47bc3c0b36f4ffb72f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_203817146cca42b4b4578e003df7eafc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e34342db2b904543ae6bcd9aa0d4eeb0"
          }
        },
        "392003aab3b2423cbc3fbdbe493e0aff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4916418a35bf454b87222070c1a7d07c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 268M/268M [00:05&lt;00:00, 49.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8cf17826c174a0c9239104a78e608d5"
          }
        },
        "203817146cca42b4b4578e003df7eafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e34342db2b904543ae6bcd9aa0d4eeb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4916418a35bf454b87222070c1a7d07c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8cf17826c174a0c9239104a78e608d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN5U7W0b7X-b"
      },
      "source": [
        "This script generates the MTL models that can be found at: https://drive.google.com/drive/folders/1s5netzimpnld-TMo3aZzyggBViERF3yT?usp=sharing \n",
        "\n",
        "Required data to run this script:\n",
        "- subjectivity_dataset.xlsx\n",
        "- labeled_dataset_cs.xlsx\n",
        "- Reddit_dataset.csv\n",
        "- STS-B.xlsx\n",
        "- IMDB_train.pt (TensorDataset, already preprocessed --> saves time)\n",
        "- IMDB_test.pt (TensorDataset)\n",
        "- Wiki_train.pt (TensorDataset)\n",
        "- Wiki_test.pt (TensorDataset)\n",
        "- SNLI_train.pt (TensorDataset)\n",
        "- SNLI_test.pt (TensorDataset)\n",
        "- classifier.weight.pt (we initialize all classifiers with the same parameters for the sake of comparability)\n",
        "- classifier.bias.pt (bias for classifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EW98CVr7bMX",
        "outputId": "411e60e8-8e5b-4223-97ed-10d8897c5d56"
      },
      "source": [
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import io\n",
        "import random\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score,f1_score,precision_score,recall_score,accuracy_score\n",
        "import transformers\n",
        "from transformers import AutoTokenizer,DistilBertTokenizer,DistilBertModel, AdamW,DistilBertForSequenceClassification,DistilBertConfig\n",
        "from torch.utils.data import DataLoader,TensorDataset,SequentialSampler,ConcatDataset,RandomSampler"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.5MB 7.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 47.5MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u35Wzqg48tR7"
      },
      "source": [
        "#connect to drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMQo98GZfSlQ"
      },
      "source": [
        "#split train dataset into train, validation and test sets\n",
        "def train_test (text,labels,test_size):\n",
        "  train_text, test_text, train_labels, test_labels = train_test_split(text, \n",
        "                                                                    labels, \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=test_size,\n",
        "                                                                    stratify=labels)\n",
        "  return train_text, test_text, train_labels, test_labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY5iqRlwfYd1"
      },
      "source": [
        "#function to tokenize sentences\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer_pairs = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize(sentences,labels,max_length = None):\n",
        "  \"tokenizes input and returns tokenized input + labels as tensors\"\n",
        "\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  for text in sentences.to_list():\n",
        "      encodings = tokenizer.encode_plus(text,add_special_tokens = True,max_length = max_length\n",
        "                                        ,truncation = True, padding = 'max_length',return_attention_mask = True)\n",
        "      input_ids.append(encodings['input_ids'])\n",
        "      attention_masks.append(encodings['attention_mask'])\n",
        "\n",
        "  return torch.tensor(input_ids),torch.tensor(attention_masks),torch.tensor(labels.to_list())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMNWDii_0yDI"
      },
      "source": [
        "# function to tokenize sentence pairs\n",
        "def tokenize_pairs(batch1,batch2,labels):\n",
        "\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  for sent1,sent2 in zip(batch1.to_list(),batch2.to_list()):\n",
        "    encodings = tokenizer.encode_plus(str(sent1),str(sent2),add_special_tokens=True,truncation=True,max_length = 512,padding='max_length')\n",
        "    input_ids.append(encodings['input_ids'])\n",
        "    attention_masks.append(encodings['attention_mask'])\n",
        "\n",
        "  return torch.tensor(input_ids),torch.tensor(attention_masks),torch.tensor(labels.to_list())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYLxdLbofq1A"
      },
      "source": [
        "# get predictions for test data\n",
        "def predict(model,dataloader):\n",
        "\n",
        "  predictions = []\n",
        "  for batch in dataloader:\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    with torch.no_grad():\n",
        "      output = model(sent_id, attention_mask=mask,labels = labels)\n",
        "      preds = output[1]\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      predictions.append(np.argmax(preds, axis = 1).flatten())\n",
        "\n",
        "  #merge sublists of predictions\n",
        "  predictions = [label for batch in predictions for label in batch]\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8wGoA61M4gn"
      },
      "source": [
        "#set seed\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)   \n",
        "random.seed(0)    \n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYHCogOqdfvC"
      },
      "source": [
        "#read data (Note: IMDB,SNLI, and Wikipedia data are directly loaded as TensorDatasets since preprocessing takes some time with these corpora)\n",
        "#df1\n",
        "df_subj = pd.read_excel('/content/drive/MyDrive/Masterthesis/Data/subjectivity_dataset.xlsx')\n",
        "\n",
        "#df2\n",
        "df_bias = pd.read_excel(\"/content/drive/MyDrive/Masterthesis/Data/labeled_dataset_cs.xlsx\")\n",
        "df_bias = df_bias[df_bias.Label_bias != 'No agreement']\n",
        "df_bias['Label_bias_0-1'] = df_bias['Label_bias'].map({'Biased  ':1,'Biased':1, 'Non-biased':0})\n",
        "df_bias['factual'] = df_bias['Label_opinion'].map( {'Entirely factual':0, 'Somewhat factual but also opinionated':1,\n",
        "                                          'Expresses writerâ€™s opinion':2,\"Expresses writer's opinion\":2})\n",
        "df_bias = df_bias[['sentence','Label_bias_0-1','factual']]\n",
        "\n",
        "#df3\n",
        "df_sts = pd.read_excel('/content/drive/MyDrive/Masterthesis/Data/STS-B.xlsx')\n",
        "\n",
        "#df4\n",
        "df_reddit = pd.read_csv('/content/drive/MyDrive/Masterthesis/Data/Reddit_dataset.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkb84DERme9r"
      },
      "source": [
        "#train test split + tokenization\n",
        "\n",
        "#df subj\n",
        "train_text, test_text, train_labels, test_labels = train_test(df_subj['text'],df_subj['label_encoded'],0.2)\n",
        "train_input_ids,train_attention_masks,train_y = tokenize(train_text,train_labels)\n",
        "test_input_ids,test_attention_masks,test_y = tokenize(test_text,test_labels)\n",
        "train_data_subj = TensorDataset(train_input_ids, train_attention_masks, train_y)\n",
        "test_data_subj = TensorDataset(test_input_ids, test_attention_masks, test_y)\n",
        "\n",
        "##df bias\n",
        "train_text, test_text, train_labels, test_labels = train_test(df_bias['sentence'], df_bias['Label_bias_0-1'],0.2)\n",
        "train_input_ids,train_attention_masks,train_y = tokenize(train_text, train_labels,max_length=119)\n",
        "test_input_ids,test_attention_masks,test_y = tokenize(test_text,test_labels,max_length=119) \n",
        "train_data_bias = TensorDataset(train_input_ids, train_attention_masks, train_y)\n",
        "test_data_bias = TensorDataset(test_input_ids, test_attention_masks, test_y)\n",
        "\n",
        "#df sts\n",
        "train_data,test_data = train_test_split(df_sts,random_state=2018,test_size = 0.2)\n",
        "train_input_ids,train_attention_masks,train_y = tokenize_pairs(train_data['sent1'],train_data['sent2'],train_data['sim_label'])\n",
        "test_input_ids,test_attention_masks,test_y = tokenize_pairs(test_data['sent1'],test_data['sent2'],test_data['sim_label'])\n",
        "train_data_sts = TensorDataset(train_input_ids, train_attention_masks, train_y)\n",
        "test_data_sts = TensorDataset(test_input_ids, test_attention_masks, test_y)\n",
        "\n",
        "#df IMDB\n",
        "# train_data_IMDB = torch.load('/content/drive/MyDrive/Masterthesis/Data/IMDB_train.pt')\n",
        "# test_data_IMDB = torch.load('/content/drive/MyDrive/Masterthesis/Data/IMDB_test.pt')\n",
        "\n",
        "#df SNLI\n",
        "train_data_SNLI = torch.load('/content/drive/MyDrive/Masterthesis/Data/SNLI_train.pt')\n",
        "test_data_SNLI = torch.load('/content/drive/MyDrive/Masterthesis/Data/SNLI_test.pt')\n",
        "\n",
        "#df wiki\n",
        "train_data_wiki = torch.load('/content/drive/MyDrive/Masterthesis/Data/Wiki_train.pt')\n",
        "test_data_wiki = torch.load('/content/drive/MyDrive/Masterthesis/Data/Wiki_test.pt')\n",
        "\n",
        "#df reddit\n",
        "train_data,test_data = train_test_split(df_reddit,random_state=2018,test_size = 0.2)\n",
        "train_input_ids,train_attention_masks,train_y = tokenize(train_data['body'],train_data['usVSthem_scale'])\n",
        "test_input_ids,test_attention_masks,test_y = tokenize(test_data['body'],test_data['usVSthem_scale'])\n",
        "train_data_reddit = TensorDataset(train_input_ids, train_attention_masks, train_y)\n",
        "test_data_reddit = TensorDataset(test_input_ids, test_attention_masks, test_y)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3aj7vxNJTiM"
      },
      "source": [
        "#define dataloader and epochs\n",
        "epochs = 4\n",
        "batch_size = 32\n",
        "\n",
        "train_sampler1 = RandomSampler(train_data_bias)\n",
        "#train_sampler2 = RandomSampler(train_data_IMDB)\n",
        "train_sampler3 = RandomSampler(train_data_subj)\n",
        "train_sampler4 = RandomSampler(train_data_wiki)\n",
        "train_sampler5 = RandomSampler(train_data_sts)\n",
        "train_sampler6 = RandomSampler(train_data_SNLI)\n",
        "train_sampler7 = RandomSampler(train_data_reddit)\n",
        "\n",
        "test_sampler1 = RandomSampler(test_data_bias)\n",
        "#test_sampler2 = RandomSampler(test_data_IMDB)\n",
        "test_sampler3 = RandomSampler(test_data_subj)\n",
        "test_sampler4 = RandomSampler(test_data_wiki)\n",
        "test_sampler5 = RandomSampler(test_data_sts)\n",
        "test_sampler6 = RandomSampler(test_data_SNLI)\n",
        "test_sampler7 = RandomSampler(test_data_reddit)\n",
        "\n",
        "train_dataloader1 = DataLoader(train_data_bias,sampler= train_sampler1, batch_size=batch_size)\n",
        "#train_dataloader2 = DataLoader(train_data_IMDB,sampler= train_sampler2, batch_size=batch_size)\n",
        "train_dataloader3 = DataLoader(train_data_subj,sampler= train_sampler3, batch_size=batch_size)\n",
        "train_dataloader4 = DataLoader(train_data_wiki,sampler= train_sampler4, batch_size=batch_size)\n",
        "train_dataloader5 = DataLoader(train_data_sts,sampler= train_sampler5, batch_size=batch_size)\n",
        "train_dataloader6 = DataLoader(train_data_SNLI,sampler= train_sampler6, batch_size=batch_size)\n",
        "train_dataloader7 = DataLoader(train_data_reddit,sampler= train_sampler7, batch_size=batch_size)\n",
        "\n",
        "test_dataloader1 = DataLoader(test_data_bias,sampler= test_sampler1, batch_size=batch_size)\n",
        "#test_dataloader2 = DataLoader(test_data_IMDB,sampler= test_sampler2, batch_size=batch_size)\n",
        "test_dataloader3 = DataLoader(test_data_subj,sampler= test_sampler3, batch_size=batch_size)\n",
        "test_dataloader4 = DataLoader(test_data_wiki,sampler= test_sampler4, batch_size=batch_size)\n",
        "test_dataloader5 = DataLoader(test_data_sts,sampler= test_sampler5, batch_size=batch_size)\n",
        "test_dataloader6 = DataLoader(test_data_SNLI,sampler= test_sampler6, batch_size=batch_size)\n",
        "test_dataloader7 = DataLoader(test_data_reddit,sampler= test_sampler7, batch_size=batch_size)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsDIQSUiJ74g"
      },
      "source": [
        "#concatenate dataloaders and shuffe batches\n",
        "train_dataloader = []\n",
        "for dataloader in [train_dataloader1,train_dataloader3,train_dataloader4,train_dataloader5,train_dataloader6,train_dataloader7]:\n",
        "  for batch in dataloader:\n",
        "    train_dataloader.append(batch)\n",
        "random.shuffle(train_dataloader)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_4XpdzbwhMP"
      },
      "source": [
        "test_dataloader = []\n",
        "for dataloader in [test_dataloader1,test_dataloader3,train_dataloader4,train_dataloader5,test_dataloader6,test_dataloader7]:\n",
        "  for batch in dataloader:\n",
        "    test_dataloader.append(batch)\n",
        "random.shuffle(test_dataloader)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_9S6NvBe-fO"
      },
      "source": [
        "#define loss\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "MSE = nn.MSELoss()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9qI1SEdbHMt"
      },
      "source": [
        "#create MTL model\n",
        "\n",
        "class DistilBertClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBertClass, self).__init__()\n",
        "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.vocab_transform = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "        self.classifier1 = nn.Linear(768,2)\n",
        "        self.classifier2 = nn.Linear(768,3)\n",
        "        self.regression = nn.Linear(768,1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,labels):\n",
        "        output_1 = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.vocab_transform(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "\n",
        "        if len(labels.unique()) == 2: #binary classification\n",
        "          output = self.classifier1(pooler)\n",
        "          loss = cross_entropy(output,labels)\n",
        "\n",
        "        if len(labels.unique()) > 3:#regression\n",
        "          output = self.regression(pooler)\n",
        "          loss = MSE(output,labels.view(-1,1))\n",
        "\n",
        "        if len(labels.unique()) == 3:#multi-label classification\n",
        "          output = self.classifier2(pooler)\n",
        "          loss = cross_entropy(output,labels.long())\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "135ae31a37ff48e0aba2c851916292f4",
            "fba165ba3a1a4a119622b2ed30959a30",
            "915f955db1264c47bc3c0b36f4ffb72f",
            "392003aab3b2423cbc3fbdbe493e0aff",
            "203817146cca42b4b4578e003df7eafc",
            "e34342db2b904543ae6bcd9aa0d4eeb0",
            "4916418a35bf454b87222070c1a7d07c",
            "b8cf17826c174a0c9239104a78e608d5"
          ]
        },
        "id": "i2BwHSj6cDri",
        "outputId": "ce874d8c-55db-4700-8e5a-706e38b502f6"
      },
      "source": [
        "#instantiate model and pass it to GPU\n",
        "device = torch.device('cuda')\n",
        "model_dbert = DistilBertClass()\n",
        "model_dbert.to(device)\n",
        "optim_dbert = AdamW(model_dbert.parameters(), lr=1e-5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "135ae31a37ff48e0aba2c851916292f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o79JiLrW5MfB"
      },
      "source": [
        "#train function\n",
        "def train(dataloader):\n",
        "\n",
        "  model_dbert.train()\n",
        "  total_loss = 0\n",
        "  counter = 0\n",
        "\n",
        "  for index,batch in enumerate(dataloader):\n",
        "    counter += 1\n",
        "    sys.stdout.write('\\r Batch {}/{}'.format(counter,len(dataloader)))\n",
        "    optim_dbert.zero_grad()\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    loss = model_dbert(sent_id, attention_mask=mask,labels = labels)\n",
        "    total_loss = total_loss+loss.item()\n",
        "    loss.backward()\n",
        "    optim_dbert.step()\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache\n",
        "    del batch,sent_id,mask,labels\n",
        "    \n",
        "  avg_loss = total_loss / len(dataloader)\n",
        "  return avg_loss"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GYQcOX_fhjv"
      },
      "source": [
        "#test function\n",
        "\n",
        "def validate(dataloader):\n",
        "    model_dbert.eval()\n",
        "    total_loss = 0\n",
        "    print(\"\\nValidating...\")\n",
        "    counter = 0\n",
        "    for batch in dataloader:\n",
        "      counter +=1\n",
        "      batch = [r.to(device) for r in batch]\n",
        "      sent_id, mask, labels = batch\n",
        "\n",
        "      with torch.no_grad():\n",
        "        loss = model_dbert(sent_id, attention_mask=mask,labels = labels)\n",
        "        total_loss = total_loss+loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader) \n",
        "    return avg_loss"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Est1Bn9UAb3w"
      },
      "source": [
        "#train/validate function\n",
        "\n",
        "def train_validate(train_dataloader,test_dataloader):\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  # empty lists to store training and validation loss of each epoch\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "\n",
        "  #for each epoch\n",
        "  for epoch in range(epochs):\n",
        "      \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss = train(train_dataloader)\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "    #evaluate model\n",
        "    valid_loss = validate(test_dataloader)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model_dbert.state_dict(), '/content/drive/MyDrive/Masterthesis/pytorch_model.bin') #insert path here\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTg22-3fbSI-",
        "outputId": "4138b51b-f967-47db-b577-5525134a2b77"
      },
      "source": [
        "#apply training and validation\n",
        "train_validate(train_dataloader,test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 4\n",
            " Batch 3235/3235\n",
            "Validating...\n",
            "\n",
            "Training Loss: 0.653\n",
            "Validation Loss: 0.542\n",
            "\n",
            " Epoch 2 / 4\n",
            " Batch 3235/3235\n",
            "Validating...\n",
            "\n",
            "Training Loss: 0.503\n",
            "Validation Loss: 0.462\n",
            "\n",
            " Epoch 3 / 4\n",
            " Batch 3235/3235\n",
            "Validating...\n",
            "\n",
            "Training Loss: 0.412\n",
            "Validation Loss: 0.408\n",
            "\n",
            " Epoch 4 / 4\n",
            " Batch 3235/3235\n",
            "Validating...\n",
            "\n",
            "Training Loss: 0.326\n",
            "Validation Loss: 0.385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqZNMLpwvtgF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}