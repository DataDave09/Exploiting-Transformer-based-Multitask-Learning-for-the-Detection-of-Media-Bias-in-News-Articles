{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Few_shot_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss6ek9Cow3sB"
      },
      "source": [
        "This script corresponds to section 4.6: Few-shot Learning\n",
        "\n",
        "Required data to run this script:\n",
        "- labeled_dataset_experts.xlsx\n",
        "- classifier.weight.pt (we initialize all classifiers with the same parameters for the sake of comparability)\n",
        "- classifier.bias.pt (bias for classifier)\n",
        "- IMDB+Reddit+wiki.bin (weights of MTL model)\n",
        "- few_shot_learning_MTL.json (few-shot-learning results for plotting)\n",
        "- quantity_analysis_experts.json (baseline results for plotting)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQkYOQImw1fQ",
        "outputId": "2b6123cc-6397-4ec7-d749-6f384fdc9867"
      },
      "source": [
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score,f1_score,precision_score,recall_score,accuracy_score,confusion_matrix\n",
        "import transformers\n",
        "from transformers import AdamW,DistilBertTokenizer,DistilBertModel\n",
        "from torch.utils.data import DataLoader,TensorDataset,SequentialSampler"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 48.9MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPOyWWn5vE_h"
      },
      "source": [
        "Connect to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VzgU5i7vHob",
        "outputId": "3d11bc8b-ce58-4148-e823-bea0721f3b2c"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtgG6osXvy0T"
      },
      "source": [
        "Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivQMG-58v0R6"
      },
      "source": [
        "class DistilBertClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBertClass, self).__init__()\n",
        "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.vocab_transform = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "        self.classifier = torch.nn.Linear(768,2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.vocab_transform(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5papSYrXwAb6"
      },
      "source": [
        "#load weights of best MTL pretrained news model\n",
        "weight_dict = torch.load('/content/drive/MyDrive/Masterthesis/MTL_models/IMDB+Reddit+wiki.bin')\n",
        "\n",
        "#remove classification weights + bias\n",
        "for e in ['classifier1.weight', 'classifier1.bias','classifier2.weight', 'classifier2.bias','regression.weight','regression.bias']: \n",
        "  weight_dict.pop(e)\n",
        "\n",
        "#load saved classifier weights + classifier bias --> every model is initialized with same classification layers for comparability reasons\n",
        "classifier_weights = torch.load('/content/drive/MyDrive/Masterthesis/Data/classifier.weights.pt')\n",
        "classifier_bias = torch.load('/content/drive/MyDrive/Masterthesis/Data/classifier.bias.pt')\n",
        "\n",
        "#insert predefined weights and bias into weight dict\n",
        "weight_dict['classifier.weight'] = classifier_weights\n",
        "weight_dict['classifier.bias'] = classifier_bias"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBUKr5n8vQZL"
      },
      "source": [
        "Connect to GPU (do not forget to switch on GPU under runtime settings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2n8HMRqvSLo",
        "outputId": "c144b24f-896a-42d0-dcf5-604fff6c9949"
      },
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMPL7XrTvZAd"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0hIkFgcvauF"
      },
      "source": [
        "df = pd.read_excel(\"/content/drive/MyDrive/Masterthesis/Data/labeled_dataset_experts.xlsx\")\n",
        "\n",
        "#select only cases with agreement\n",
        "df = df[df['label_bias']!= 'No agreement']\n",
        "\n",
        "#encode string variable into binary variable\n",
        "df['Label_bias_0-1'] = df['label_bias'].map({'Biased':1,'Non-biased':0})\n",
        "df = df[['text','Label_bias_0-1']]\n",
        "\n",
        "#split data in train test, so that all sub-datasets of different size are evaluated on same held-out sample\n",
        "train_data, test_data = train_test_split(df,random_state=2018,test_size=0.2,stratify=df['Label_bias_0-1'])\n",
        "\n",
        "#subsampling of train data containing samples (20%,30%,40%...) of original data\n",
        "df_20 = train_data.groupby('Label_bias_0-1', group_keys=False).apply(lambda x: x.sample(int(np.rint((0.2*len(df))*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
        "df_30 = train_data.groupby('Label_bias_0-1', group_keys=False).apply(lambda x: x.sample(int(np.rint((0.3*len(df))*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
        "df_40 = train_data.groupby('Label_bias_0-1', group_keys=False).apply(lambda x: x.sample(int(np.rint((0.4*len(df))*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
        "df_50 = train_data.groupby('Label_bias_0-1', group_keys=False).apply(lambda x: x.sample(int(np.rint((0.5*len(df))*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
        "df_60 = train_data.groupby('Label_bias_0-1', group_keys=False).apply(lambda x: x.sample(int(np.rint((0.6*len(df))*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
        "df_70 = train_data.groupby('Label_bias_0-1', group_keys=False).apply(lambda x: x.sample(int(np.rint((0.7*len(df))*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
        "df_80 = train_data.groupby('Label_bias_0-1', group_keys=False).apply(lambda x: x.sample(int(np.rint((0.8*len(df))*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
        "df_90 = train_data.groupby('Label_bias_0-1', group_keys=False).apply(lambda x: x.sample(int(np.rint((0.9*len(df))*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# create list of all df's\n",
        "dfs = [df_20,df_30,df_40,df_50,df_60,df_70,df_80,df_90,train_data]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdFMb0U2feYy"
      },
      "source": [
        "#prepare test data (it's the same for all subsamples)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "#divide data into folds\n",
        "test_text = test_data['text']\n",
        "test_labels = test_data['Label_bias_0-1']\n",
        "\n",
        "#encode\n",
        "test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True)\n",
        "\n",
        "#convert input to tensors \n",
        "test_seq = torch.tensor(test_encodings['input_ids'])\n",
        "test_mask = torch.tensor(test_encodings['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "\n",
        "# wrap tensors into one dataset\n",
        "test_data = TensorDataset(test_seq, test_mask, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC39z8GFxQtB"
      },
      "source": [
        "Define Cross-Validation,Dataloader,Epochs,Loss, and Seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7LEkeyHxP7B"
      },
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)   \n",
        "random.seed(0)    \n",
        "torch.cuda.manual_seed_all(0)\n",
        "random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "kfold = StratifiedKFold(shuffle = True,random_state=2018)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 4\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBHCXpsxc_e"
      },
      "source": [
        "Define functions for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA6Np8VLxbuF"
      },
      "source": [
        "def train(model):\n",
        "\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch in train_dataloader:\n",
        "    optim_dbert.zero_grad()\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    outputs = model(sent_id, attention_mask=mask)\n",
        "    loss = cross_entropy(outputs,labels)\n",
        "    total_loss = total_loss+loss.item()\n",
        "    loss.backward()\n",
        "    optim_dbert.step()\n",
        "\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VvtdBe3xj0h"
      },
      "source": [
        "def validate(model):\n",
        "\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "\n",
        "  print(\"\\n   Validating...\")\n",
        "\n",
        "  for batch in test_dataloader:\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(sent_id, attention_mask=mask)\n",
        "      loss = cross_entropy(outputs,labels)\n",
        "      total_loss = total_loss+loss.item()\n",
        "\n",
        "  avg_loss = total_loss / len(test_dataloader) \n",
        "\n",
        "  return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krCHuXqsxn5l"
      },
      "source": [
        "def train_validate_pred(model):\n",
        "  best_valid_loss = float('inf')\n",
        "\n",
        "  # empty lists to store training and validation loss of each epoch\n",
        "  train_losses=[]\n",
        "  valid_losses=[]\n",
        "\n",
        "  #for each epoch\n",
        "  for epoch in range(epochs):\n",
        "      \n",
        "    print('\\n   Epoch {} / {}'.format(epoch+1,epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss = train(model)\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss = validate(model)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      global model_dbert\n",
        "      torch.save(model_dbert.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    #if validation loss increases, stop training\n",
        "    elif valid_loss >= best_valid_loss:\n",
        "      print(\"\\n Validation loss not decreased, Model of previous epoch saved\")\n",
        "      break\n",
        "    \n",
        "    print(f'\\n    Training Loss: {train_loss:.3f}')\n",
        "    print(f'    Validation Loss: {valid_loss:.3f}')\n",
        "  \n",
        "  #predict\n",
        "  path = 'saved_weights.pt'\n",
        "  model_dbert.load_state_dict(torch.load(path))\n",
        "  with torch.no_grad():\n",
        "    preds = model_dbert(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  preds = np.argmax(preds, axis = 1)\n",
        "  \n",
        "  #save results\n",
        "  loss.append(best_valid_loss)\n",
        "  acc.append(accuracy_score(test_y,preds))\n",
        "  auc.append(roc_auc_score(test_y,preds))\n",
        "  micro_F1.append(f1_score(test_y,preds,average='micro'))\n",
        "  macro_F1_weighted.append(f1_score(test_y,preds,average='weighted'))\n",
        "  binary_F1.append(f1_score(test_y,preds,average='binary'))\n",
        "  precision.append(precision_score(test_y,preds))\n",
        "  recall.append(recall_score(test_y,preds))\n",
        "  conf_matrix = confusion_matrix(test_y, preds)\n",
        "  conf_matrices.append(conf_matrix)\n",
        "\n",
        "  del model_dbert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8JSM2LfxvRA"
      },
      "source": [
        "Implement cross validation, train/validate the model, and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eohdRZ05xogi",
        "outputId": "72fcc38d-01d7-42a6-8abe-10221dff2a51"
      },
      "source": [
        "#apply cross-validation for all dataframes\n",
        "loss_lst = []\n",
        "#acc_lst = []\n",
        "micro_f1_lst = []\n",
        "macro_f1_lst = []\n",
        "binary_f1_lst = []\n",
        "prec_lst = []\n",
        "recall_lst = []\n",
        "\n",
        "for df in dfs:\n",
        "  print(\"\\n New Dataframe\")\n",
        "\n",
        "  #create new lists to store metrics of every cv repetition\n",
        "  loss = []\n",
        "  acc = []\n",
        "  auc = []\n",
        "  micro_F1 = []\n",
        "  macro_F1_weighted = []\n",
        "  binary_F1 = []\n",
        "  precision = []\n",
        "  recall = []\n",
        "  conf_matrices = []\n",
        "\n",
        "  #apply cross-validation and train/validate/predict for each fold\n",
        "  for fold, (train_index, test_index) in enumerate(kfold.split(df['text'],df['Label_bias_0-1'])):\n",
        "    sys.stdout.write('\\n \\r Fold {} / {}\\n'.format(fold+1,kfold.get_n_splits()))\n",
        "\n",
        "    #divide data into folds\n",
        "    train_text = df['text'].iloc[train_index]\n",
        "    train_labels = df['Label_bias_0-1'].iloc[train_index]\n",
        "\n",
        "    #encode\n",
        "    train_encodings = tokenizer(train_text.tolist(), truncation=True, padding=True)\n",
        "\n",
        "    #convert input to tensors \n",
        "    train_seq = torch.tensor(train_encodings['input_ids'])\n",
        "    train_mask = torch.tensor(train_encodings['attention_mask'])\n",
        "    train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "    # wrap tensors into one dataset\n",
        "    train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "    #define dataloader\n",
        "    train_sampler = SequentialSampler(train_data)\n",
        "    test_sampler = SequentialSampler(test_data)\n",
        "    train_dataloader = DataLoader(train_data,sampler= train_sampler, batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(test_data,sampler = test_sampler, batch_size=batch_size)\n",
        "\n",
        "    #create model and optimizer\n",
        "    model_dbert = DistilBertClass()\n",
        "    model_dbert.load_state_dict(weight_dict)\n",
        "    model_dbert.to(device)\n",
        "    optim_dbert = AdamW(model_dbert.parameters(), lr=1e-5)\n",
        "\n",
        "    #train_validate_predict\n",
        "    train_validate_pred(model=model_dbert)\n",
        "  \n",
        "  #compute final cross validated scores\n",
        "  cv_loss = sum(loss)/len(loss)\n",
        "  cv_micro_f1 = sum(micro_F1)/len(micro_F1)\n",
        "  cv_macro_f1 = sum(macro_F1_weighted)/len(macro_F1_weighted)\n",
        "  cv_binary_f1 = sum(binary_F1)/len(binary_F1)\n",
        "  cv_prec = sum(precision)/len(precision)\n",
        "  cv_recall = sum(recall)/len(recall)\n",
        "\n",
        "  #append scores to list of scores for later plotting\n",
        "  loss_lst.append(cv_loss)\n",
        "  micro_f1_lst.append(cv_micro_f1)\n",
        "  macro_f1_lst.append(cv_macro_f1)\n",
        "  binary_f1_lst.append(cv_binary_f1)\n",
        "  prec_lst.append(cv_prec)\n",
        "  recall_lst.append(cv_recall)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " New Dataframe\n",
            "\n",
            " \r Fold 1 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.690\n",
            "    Validation Loss: 0.678\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.667\n",
            "    Validation Loss: 0.664\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.639\n",
            "    Validation Loss: 0.646\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.598\n",
            "    Validation Loss: 0.618\n",
            "\n",
            " Fold 2 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.692\n",
            "    Validation Loss: 0.679\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.664\n",
            "    Validation Loss: 0.663\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.637\n",
            "    Validation Loss: 0.642\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.582\n",
            "    Validation Loss: 0.608\n",
            "\n",
            " Fold 3 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.699\n",
            "    Validation Loss: 0.679\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.668\n",
            "    Validation Loss: 0.666\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.645\n",
            "    Validation Loss: 0.649\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.614\n",
            "    Validation Loss: 0.625\n",
            "\n",
            " Fold 4 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.687\n",
            "    Validation Loss: 0.677\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.659\n",
            "    Validation Loss: 0.660\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.619\n",
            "    Validation Loss: 0.635\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.568\n",
            "    Validation Loss: 0.596\n",
            "\n",
            " Fold 5 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.689\n",
            "    Validation Loss: 0.679\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.669\n",
            "    Validation Loss: 0.664\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.639\n",
            "    Validation Loss: 0.643\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.594\n",
            "    Validation Loss: 0.610\n",
            "\n",
            " New Dataframe\n",
            "\n",
            " Fold 1 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.691\n",
            "    Validation Loss: 0.672\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.648\n",
            "    Validation Loss: 0.644\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.585\n",
            "    Validation Loss: 0.592\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.477\n",
            "    Validation Loss: 0.529\n",
            "\n",
            " Fold 2 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.689\n",
            "    Validation Loss: 0.673\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.650\n",
            "    Validation Loss: 0.648\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.600\n",
            "    Validation Loss: 0.605\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.499\n",
            "    Validation Loss: 0.545\n",
            "\n",
            " Fold 3 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.683\n",
            "    Validation Loss: 0.673\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.654\n",
            "    Validation Loss: 0.647\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.604\n",
            "    Validation Loss: 0.598\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.510\n",
            "    Validation Loss: 0.532\n",
            "\n",
            " Fold 4 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.685\n",
            "    Validation Loss: 0.670\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.641\n",
            "    Validation Loss: 0.639\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.586\n",
            "    Validation Loss: 0.582\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.481\n",
            "    Validation Loss: 0.515\n",
            "\n",
            " Fold 5 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.692\n",
            "    Validation Loss: 0.674\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.659\n",
            "    Validation Loss: 0.651\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.611\n",
            "    Validation Loss: 0.608\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.519\n",
            "    Validation Loss: 0.543\n",
            "\n",
            " New Dataframe\n",
            "\n",
            " Fold 1 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.691\n",
            "    Validation Loss: 0.674\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.659\n",
            "    Validation Loss: 0.649\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.608\n",
            "    Validation Loss: 0.588\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.507\n",
            "    Validation Loss: 0.531\n",
            "\n",
            " Fold 2 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.694\n",
            "    Validation Loss: 0.672\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.657\n",
            "    Validation Loss: 0.646\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.613\n",
            "    Validation Loss: 0.587\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.513\n",
            "    Validation Loss: 0.527\n",
            "\n",
            " Fold 3 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.689\n",
            "    Validation Loss: 0.671\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.649\n",
            "    Validation Loss: 0.637\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.582\n",
            "    Validation Loss: 0.558\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.457\n",
            "    Validation Loss: 0.516\n",
            "\n",
            " Fold 4 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.691\n",
            "    Validation Loss: 0.671\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.652\n",
            "    Validation Loss: 0.644\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.611\n",
            "    Validation Loss: 0.581\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.508\n",
            "    Validation Loss: 0.525\n",
            "\n",
            " Fold 5 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.689\n",
            "    Validation Loss: 0.673\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.650\n",
            "    Validation Loss: 0.642\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.601\n",
            "    Validation Loss: 0.573\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.496\n",
            "    Validation Loss: 0.526\n",
            "\n",
            " New Dataframe\n",
            "\n",
            " Fold 1 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.677\n",
            "    Validation Loss: 0.659\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.616\n",
            "    Validation Loss: 0.594\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.490\n",
            "    Validation Loss: 0.521\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.353\n",
            "    Validation Loss: 0.517\n",
            "\n",
            " Fold 2 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.683\n",
            "    Validation Loss: 0.661\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.625\n",
            "    Validation Loss: 0.599\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.511\n",
            "    Validation Loss: 0.507\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.390\n",
            "    Validation Loss: 0.490\n",
            "\n",
            " Fold 3 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.681\n",
            "    Validation Loss: 0.661\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.625\n",
            "    Validation Loss: 0.599\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.505\n",
            "    Validation Loss: 0.512\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.381\n",
            "    Validation Loss: 0.501\n",
            "\n",
            " Fold 4 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.681\n",
            "    Validation Loss: 0.659\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.624\n",
            "    Validation Loss: 0.594\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.516\n",
            "    Validation Loss: 0.509\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.398\n",
            "    Validation Loss: 0.498\n",
            "\n",
            " Fold 5 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.680\n",
            "    Validation Loss: 0.660\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.619\n",
            "    Validation Loss: 0.592\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.494\n",
            "    Validation Loss: 0.514\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " New Dataframe\n",
            "\n",
            " Fold 1 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.673\n",
            "    Validation Loss: 0.650\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.582\n",
            "    Validation Loss: 0.544\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.435\n",
            "    Validation Loss: 0.512\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 2 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.676\n",
            "    Validation Loss: 0.653\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.592\n",
            "    Validation Loss: 0.552\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.460\n",
            "    Validation Loss: 0.534\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 3 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.674\n",
            "    Validation Loss: 0.658\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.612\n",
            "    Validation Loss: 0.569\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.471\n",
            "    Validation Loss: 0.545\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.371\n",
            "    Validation Loss: 0.533\n",
            "\n",
            " Fold 4 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.675\n",
            "    Validation Loss: 0.653\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.588\n",
            "    Validation Loss: 0.548\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.451\n",
            "    Validation Loss: 0.519\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 5 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.677\n",
            "    Validation Loss: 0.657\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.601\n",
            "    Validation Loss: 0.562\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.448\n",
            "    Validation Loss: 0.533\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.342\n",
            "    Validation Loss: 0.531\n",
            "\n",
            " New Dataframe\n",
            "\n",
            " Fold 1 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.677\n",
            "    Validation Loss: 0.646\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.585\n",
            "    Validation Loss: 0.533\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 2 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.673\n",
            "    Validation Loss: 0.642\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.575\n",
            "    Validation Loss: 0.517\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.423\n",
            "    Validation Loss: 0.479\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 3 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.677\n",
            "    Validation Loss: 0.646\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.583\n",
            "    Validation Loss: 0.525\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.439\n",
            "    Validation Loss: 0.490\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 4 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.672\n",
            "    Validation Loss: 0.643\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.574\n",
            "    Validation Loss: 0.524\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.434\n",
            "    Validation Loss: 0.498\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 5 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.673\n",
            "    Validation Loss: 0.636\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.569\n",
            "    Validation Loss: 0.510\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.439\n",
            "    Validation Loss: 0.480\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " New Dataframe\n",
            "\n",
            " Fold 1 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.673\n",
            "    Validation Loss: 0.637\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.553\n",
            "    Validation Loss: 0.498\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.406\n",
            "    Validation Loss: 0.478\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 2 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.675\n",
            "    Validation Loss: 0.644\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.571\n",
            "    Validation Loss: 0.521\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.432\n",
            "    Validation Loss: 0.510\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 3 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.670\n",
            "    Validation Loss: 0.632\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.550\n",
            "    Validation Loss: 0.515\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.403\n",
            "    Validation Loss: 0.499\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 4 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.677\n",
            "    Validation Loss: 0.642\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.559\n",
            "    Validation Loss: 0.505\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.421\n",
            "    Validation Loss: 0.486\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 5 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.672\n",
            "    Validation Loss: 0.635\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.555\n",
            "    Validation Loss: 0.511\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.421\n",
            "    Validation Loss: 0.508\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " New Dataframe\n",
            "\n",
            " Fold 1 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.663\n",
            "    Validation Loss: 0.616\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.522\n",
            "    Validation Loss: 0.485\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.396\n",
            "    Validation Loss: 0.472\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 2 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.655\n",
            "    Validation Loss: 0.596\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.499\n",
            "    Validation Loss: 0.499\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.364\n",
            "    Validation Loss: 0.494\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 3 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.663\n",
            "    Validation Loss: 0.613\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.525\n",
            "    Validation Loss: 0.505\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.376\n",
            "    Validation Loss: 0.500\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 4 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.665\n",
            "    Validation Loss: 0.624\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.529\n",
            "    Validation Loss: 0.499\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.397\n",
            "    Validation Loss: 0.490\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 5 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.661\n",
            "    Validation Loss: 0.605\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.524\n",
            "    Validation Loss: 0.502\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.378\n",
            "    Validation Loss: 0.493\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " New Dataframe\n",
            "\n",
            " Fold 1 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.662\n",
            "    Validation Loss: 0.606\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.506\n",
            "    Validation Loss: 0.485\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 2 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.665\n",
            "    Validation Loss: 0.616\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.517\n",
            "    Validation Loss: 0.494\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 3 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.663\n",
            "    Validation Loss: 0.610\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.523\n",
            "    Validation Loss: 0.486\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.385\n",
            "    Validation Loss: 0.482\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 4 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.668\n",
            "    Validation Loss: 0.623\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.538\n",
            "    Validation Loss: 0.506\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.398\n",
            "    Validation Loss: 0.491\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n",
            "\n",
            " Fold 5 / 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "   Epoch 1 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.666\n",
            "    Validation Loss: 0.611\n",
            "\n",
            "   Epoch 2 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.523\n",
            "    Validation Loss: 0.499\n",
            "\n",
            "   Epoch 3 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            "    Training Loss: 0.370\n",
            "    Validation Loss: 0.497\n",
            "\n",
            "   Epoch 4 / 4\n",
            "\n",
            "   Validating...\n",
            "\n",
            " Validation loss not decreased, Model of previous epoch saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAbAsXuJyqzu"
      },
      "source": [
        "Save metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT5pFIJLypXJ"
      },
      "source": [
        "# few_shot_learning_MTL = {\"loss\":loss_lst,\"micro_f1\":micro_f1_lst,\"binary_f1\":binary_f1_lst,\"macro_f1\":macro_f1_lst,\"prec\":prec_lst,\"recall\":recall_lst}\n",
        "\n",
        "# #write\n",
        "# with open('/content/drive/MyDrive/Masterthesis/Results/few_shot_learning_MTL.json', 'w') as f:\n",
        "#   json.dump(few_shot_learning_MTL, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BMJeG1UL7dM"
      },
      "source": [
        "#read\n",
        "with open('/content/drive/MyDrive/Masterthesis/Results/few_shot_learning_MTL.json') as f:\n",
        "  metrics_MTL= json.load(f)\n",
        "with open('/content/drive/MyDrive/Masterthesis/Results/quanity_analysis_experts.json') as f:\n",
        "  metrics_baseline= json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtFqnQvqy1Yp"
      },
      "source": [
        "Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "5WQWB3-Vy1_9",
        "outputId": "51ec59f2-581c-44a1-c2d4-20ed30c9d7fd"
      },
      "source": [
        "ind = [247,371,494,618,742,865,989,1112,1236]\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(ind,metrics_MTL['macro_f1'],label = 'Macro F1 MTL')\n",
        "plt.plot(ind,metrics_MTL['loss'],label = 'Loss MTL')\n",
        "plt.plot(ind,metrics_baseline['macro_f1'],label = 'Macro F1 BL')\n",
        "plt.plot(ind,metrics_baseline['loss'],label = 'Loss BL')\n",
        "plt.ylim(0.4,0.9)\n",
        "plt.xlabel('Subsample Size')\n",
        "plt.ylabel('Performance')\n",
        "plt.legend()\n",
        "plt.xticks([247,371,494,618,742,865,989,1112,1236])\n",
        "plt.yticks(np.arange(0.4,0.9,0.05))\n",
        "axes = plt.gca()\n",
        "axes.yaxis.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}